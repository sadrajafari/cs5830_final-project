{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, precision_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canceling the errors that would happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting TimeStamp column into Month, Day, Hour, and Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"])\n",
    "data[\"Month\"] = data[\"Timestamp\"].dt.month\n",
    "data[\"Day\"] = data[\"Timestamp\"].dt.day\n",
    "data[\"Hour\"] = data[\"Timestamp\"].dt.hour\n",
    "data[\"Minute\"] = data[\"Timestamp\"].dt.minute\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data\n",
    "new_data = new_data.drop([\"Unnamed: 0\", \"Timestamp\"], axis=1)\n",
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.dropna()\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there is any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[\"y\"].unique()\n",
    "new_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X = new_data.drop(\"y\", axis=1)\n",
    "y = new_data[\"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting top 6 features based on highest correlation with the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features = [\"R_VALUE\", \"USFLUX\", \"TOTFZ\", \"TOTUSJH\", \"TOTBSQ\", \"TOTUSJZ\"]\n",
    "\n",
    "X = new_data[features]\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = new_data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define kfold with 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average is based on Weighted approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_scorer = make_scorer(precision_score, average='weighted')\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "fScore_scorer = make_scorer(f1_score, average='weighted')\n",
    "accuracy_scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "\n",
    "accuracy_results = cross_val_score(dt, X, y, cv=kfold)\n",
    "precision_results = cross_val_score(dt, X, y, cv=kfold, scoring=precision_scorer)\n",
    "recall_results = cross_val_score(dt, X, y, cv=kfold, scoring=recall_scorer)\n",
    "fScore_results = cross_val_score(dt, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "\n",
    "print(\"Average accuracy: \" + str(np.mean(accuracy_results)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_results)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_results)))\n",
    "print(\"Average f_score: \" + str(np.mean(fScore_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "\n",
    "\n",
    "accuracy_results = cross_val_score(knn, X, y, cv=kfold)\n",
    "precision_results = cross_val_score(knn, X, y, cv=kfold, scoring=precision_scorer)\n",
    "recall_results = cross_val_score(knn, X, y, cv=kfold, scoring=recall_scorer)\n",
    "fScore_results = cross_val_score(knn, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "print(\"Average accuracy: \" + str(np.mean(accuracy_results)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_results)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_results)))\n",
    "print(\"Average f_score: \" + str(np.mean(fScore_results)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "\n",
    "\n",
    "\n",
    "accuracy_results = cross_val_score(naive_bayes, X, y, cv=kfold)\n",
    "precision_results = cross_val_score(naive_bayes, X, y, cv=kfold, scoring=precision_scorer)\n",
    "recall_results = cross_val_score(naive_bayes, X, y, cv=kfold, scoring=recall_scorer)\n",
    "fScore_results = cross_val_score(naive_bayes, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "\n",
    "print(\"Average accuracy: \" + str(np.mean(accuracy_results)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_results)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_results)))\n",
    "print(\"Average f_score: \" + str(np.mean(fScore_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "accuracy_results = cross_val_score(lr, X, y, cv=kfold)\n",
    "precision_results = cross_val_score(lr, X, y, cv=kfold, scoring=precision_scorer)\n",
    "recall_results = cross_val_score(lr, X, y, cv=kfold, scoring=recall_scorer)\n",
    "fScore_results = cross_val_score(lr, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "\n",
    "print(\"Average accuracy: \" + str(np.mean(accuracy_results)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_results)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_results)))\n",
    "print(\"Average f_score: \" + str(np.mean(fScore_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used brute force to find the best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# The commented code is brute force that calculate the best weight for each base learner based on the highest f-score macro\n",
    "# fScore_scorer = make_scorer(f1_score, average='weighted')\n",
    "# best_records =[0, 0, 0, 0, 0]\n",
    "# for i in range(1, 5):\n",
    "#     print(i)\n",
    "#     for j in range(1, 5):\n",
    "#         for k in range(1, 5):\n",
    "#             for l in range(1, 5):\n",
    "#                 ensemble_classifier = VotingClassifier(estimators=[\n",
    "#                     ('lr', lg), \n",
    "#                     ('nv', naive_bayes), \n",
    "#                     ('knn', knn),\n",
    "#                     ('dt', dt)], \n",
    "#                     voting='soft', weights=[i, j, k, l])\n",
    "#                 fScore_results = cross_val_score(ensemble_classifier, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "#                 if np.mean(fScore_results) > best_records[0]:\n",
    "#                     best_records[0] = np.mean(fScore_results)\n",
    "#                     best_records[1] = i\n",
    "#                     best_records[2] = j\n",
    "#                     best_records[3] = k\n",
    "#                     best_records[4] = l\n",
    "\n",
    "\n",
    "# print(best_records)         \n",
    "\n",
    "\n",
    "'''\n",
    "After running the above code it looks that these weights are the best based on the highest f-score: [1, 1, 3, 2]\n",
    "\n",
    "'''\n",
    "ensemble_classifier = VotingClassifier(estimators=[\n",
    "                    ('lr', lr), \n",
    "                    ('nv', naive_bayes), \n",
    "                    ('knn', knn),\n",
    "                    ('dt', dt)], \n",
    "                    voting='soft', weights=[1, 1, 3, 2])\n",
    "\n",
    "\n",
    "accuracy_results = cross_val_score(ensemble_classifier, X, y, cv=kfold)\n",
    "print(\"Average accuracy: \" + str(np.mean(accuracy_results)))\n",
    "precision_results = cross_val_score(ensemble_classifier, X, y, cv=kfold, scoring=precision_scorer)\n",
    "print(\"Average precision: \" + str(np.mean(precision_results)))\n",
    "recall_results = cross_val_score(ensemble_classifier, X, y, cv=kfold, scoring=recall_scorer)\n",
    "print(\"Average recall: \" + str(np.mean(recall_results)))\n",
    "fScore_results = cross_val_score(ensemble_classifier, X, y, cv=kfold, scoring=fScore_scorer)\n",
    "print(\"Average f_score: \" + str(np.mean(fScore_results)))\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_results))\n",
    "print(\"Precision: \" + str((precision_results)))\n",
    "print(\"Recall: \" + str((recall_results)))\n",
    "print(\"F score: \" + str((fScore_results)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
